{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark-Project\n",
    "## Author: Veera Marni\n",
    "## Email: vmarni@iu.edu\n",
    "----------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Data Description\n",
    "\n",
    "This research employed a binary variable, default payment (Yes = 1, No = 0), as the response variable. This study reviewed the literature and used the following 23 variables as explanatory variables:\n",
    "\n",
    "1. X1: Amount of the given credit (NT dollar): it includes both the individual consumer credit and his/her family (supplementary) credit. \n",
    "2. X2: Gender (1 = male; 2 = female). \n",
    "3. X3: Education (1 = graduate school; 2 = university; 3 = high school; 4 = others). \n",
    "4. X4: Marital status (1 = married; 2 = single; 3 = others). \n",
    "5. X5: Age (year). \n",
    "6. X6 - X11: History of past payment. We tracked the past monthly payment records (from April to September, 2005) as follows: X6 = the repayment status in September, 2005; X7 = the repayment status in August, 2005; . . .;X11 = the repayment status in April, 2005. The measurement scale for the repayment status is: -1 = pay duly; 1 = payment delay for one month; 2 = payment delay for two months; . . .; 8 = payment delay for eight months; 9 = payment delay for nine months and above. \n",
    "7. X12-X17: Amount of bill statement (NT dollar). X12 = amount of bill statement in September, 2005; X13 = amount of bill statement in August, 2005; . . .; X17 = amount of bill statement in April, 2005. \n",
    "8. X18-X23: Amount of previous payment (NT dollar). X18 = amount paid in September, 2005; X19 = amount paid in August, 2005; . . .;X23 = amount paid in April, 2005. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "conf = SparkConf().setAppName('MyAppName').setMaster('local')\n",
    "sc = SparkContext(conf=conf)\n",
    "\n",
    "sqlContext = SQLContext(sc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# default_card_clients_rdd = sc.textFile('./default_of_credit_card_clients.csv').map(lambda line: \n",
    "#                                                                                    line.split(','))\n",
    "default_card_clients_rdd = sc.textFile('./default_of_credit_card_clients.csv', 4)\n",
    "header = default_card_clients_rdd.first().split(',')\n",
    "header[-1] = \"next_month_payment_status\"        \n",
    "\n",
    "default_card_clients_rdd_header = default_card_clients_rdd. \\\n",
    "                                        filter(lambda line: 'ID' in line)\n",
    "default_card_clients_rdd_Noheader = default_card_clients_rdd. \\\n",
    "                                        subtract(default_card_clients_rdd_header)\n",
    "default_card_clients_rdd_Noheader = default_card_clients_rdd_Noheader.map(\n",
    "                                        lambda line: [int(val) for val in line.split(',')])\n",
    "\n",
    "feilds = [StructField(name=feild_name, dataType=IntegerType(), \n",
    "                      nullable=True) for feild_name in header]\n",
    "schema = StructType(feilds)\n",
    "\n",
    "# print header\n",
    "# print schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: integer (nullable = true)\n",
      " |-- LIMIT_BAL: integer (nullable = true)\n",
      " |-- SEX: integer (nullable = true)\n",
      " |-- EDUCATION: integer (nullable = true)\n",
      " |-- MARRIAGE: integer (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- PAY_0: integer (nullable = true)\n",
      " |-- PAY_2: integer (nullable = true)\n",
      " |-- PAY_3: integer (nullable = true)\n",
      " |-- PAY_4: integer (nullable = true)\n",
      " |-- PAY_5: integer (nullable = true)\n",
      " |-- PAY_6: integer (nullable = true)\n",
      " |-- BILL_AMT1: integer (nullable = true)\n",
      " |-- BILL_AMT2: integer (nullable = true)\n",
      " |-- BILL_AMT3: integer (nullable = true)\n",
      " |-- BILL_AMT4: integer (nullable = true)\n",
      " |-- BILL_AMT5: integer (nullable = true)\n",
      " |-- BILL_AMT6: integer (nullable = true)\n",
      " |-- PAY_AMT1: integer (nullable = true)\n",
      " |-- PAY_AMT2: integer (nullable = true)\n",
      " |-- PAY_AMT3: integer (nullable = true)\n",
      " |-- PAY_AMT4: integer (nullable = true)\n",
      " |-- PAY_AMT5: integer (nullable = true)\n",
      " |-- PAY_AMT6: integer (nullable = true)\n",
      " |-- next_month_payment_status: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfcc = sqlContext.createDataFrame(\n",
    "            data=default_card_clients_rdd_Noheader,schema=schema)\n",
    "dfcc.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|(LIMIT_BAL + ID)|\n",
      "+----------------+\n",
      "|           20001|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfcc.createOrReplaceTempView('default_card_clients_rdd_Noheader')\n",
    "\n",
    "results = sqlContext.sql(\n",
    "    \"SELECT LIMIT_BAL+ID FROM default_card_clients_rdd_Noheader where ID=1\")\n",
    "\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are generating new features which are will be useful for modelling as we will see later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=1753, LIMIT_BAL=60000, EDUCATION=2, MARRIAGE=1, DUE_AMT1=-25736, DUE_AMT2=30500, DUE_AMT3=0, DUE_AMT4=-52829, DUE_AMT5=11425, DUE_AMT6=19003, DPNM=0),\n",
       " Row(ID=8961, LIMIT_BAL=50000, EDUCATION=3, MARRIAGE=3, DUE_AMT1=24302, DUE_AMT2=21241, DUE_AMT3=16368, DUE_AMT4=8121, DUE_AMT5=8292, DUE_AMT6=8462, DPNM=0),\n",
       " Row(ID=18693, LIMIT_BAL=130000, EDUCATION=2, MARRIAGE=3, DUE_AMT1=3143, DUE_AMT2=1574, DUE_AMT3=0, DUE_AMT4=-12677, DUE_AMT5=-17905, DUE_AMT6=30582, DPNM=1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfcc.createOrReplaceTempView(\"table1\")\n",
    "\n",
    "dfcc_monthly_dues = sqlContext.sql('SELECT \\\n",
    "                    ID, LIMIT_BAL,\\\n",
    "                    EDUCATION,\\\n",
    "                    MARRIAGE, \\\n",
    "                    BILL_AMT1-PAY_AMT1 AS DUE_AMT1,\\\n",
    "                    BILL_AMT2-PAY_AMT2 AS DUE_AMT2,\\\n",
    "                    BILL_AMT3-PAY_AMT3 AS DUE_AMT3,\\\n",
    "                    BILL_AMT4-PAY_AMT4 AS DUE_AMT4,\\\n",
    "                    BILL_AMT5-PAY_AMT5 AS DUE_AMT5,\\\n",
    "                    BILL_AMT6-PAY_AMT6 AS DUE_AMT6, \\\n",
    "                    next_month_payment_status AS DPNM from table1'\n",
    "                                   )\n",
    "\n",
    "dfcc_monthly_dues.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.0,[1753.0,60000.0,2.0,1.0,-25736.0,30500.0,0.0,-52829.0,11425.0,19003.0])\n"
     ]
    }
   ],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "\n",
    "def parserPoint(line):\n",
    "    return LabeledPoint(line[-1], line[:-1])\n",
    "\n",
    "dfcc_monthly_dues_rdd = dfcc_monthly_dues.rdd\n",
    "parsed_dfcc_monthly_dues = dfcc_monthly_dues_rdd.map(\n",
    "                        lambda line: parserPoint(line))\n",
    "print parsed_dfcc_monthly_dues.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24091\n",
      "5909\n",
      "<class 'pyspark.rdd.PipelinedRDD'>\n",
      "(0.0,[1753.0,60000.0,2.0,1.0,-25736.0,30500.0,0.0,-52829.0,11425.0,19003.0])\n"
     ]
    }
   ],
   "source": [
    "train, test = parsed_dfcc_monthly_dues.randomSplit(\n",
    "                                        weights=[0.8,0.2])\n",
    "print train.count()\n",
    "print test.count()\n",
    "print type(train)\n",
    "print train.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Logistic Regersion as this is a binary classificaiton problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "lr = LogisticRegressionWithLBFGS.train(\n",
    "            data=train, intercept=True, iterations=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:  0.870893134514\n",
      "Weights:  [-1.89203185143e-05,-2.23713734051e-06,-0.264932385385,-0.581250940679,-1.61309015366e-09,1.11197994454e-07,2.73880992407e-08,1.85438746928e-07,1.75456075931e-07,2.17450429914e-07]\n"
     ]
    }
   ],
   "source": [
    "print \"Intercept: \",lr.intercept\n",
    "print \"Weights: \",lr.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Err:  0.217464884075\n"
     ]
    }
   ],
   "source": [
    "predictions = test.map(lambda p: \n",
    "                       (p.label, lr.predict(p.features)))\n",
    "test_err = predictions.filter(\n",
    "                    lambda (v,p): v!=p).count()/float(test.count())\n",
    "print \"Test Err: \",test_err            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypothesis testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final features:  ['LIMIT_BAL', 'EDUCATION', 'MARRIAGE', 'DUE_AMT1', 'DUE_AMT2', 'DUE_AMT3', 'DUE_AMT4', 'DUE_AMT5', 'DUE_AMT6', 'DPNM']\n",
      "\n",
      "\n",
      "Train:  23989\n",
      "Test :  6011\n",
      "sample --\n",
      "(0.0,[60000.0,2.0,1.0,-25736.0,30500.0,0.0,-52829.0,11425.0,19003.0])\n"
     ]
    }
   ],
   "source": [
    "def parserPoint(line):\n",
    "    return LabeledPoint(line[-1], line[:-1])\n",
    "\n",
    "col = dfcc_monthly_dues.columns\n",
    "drop_col = ['ID']\n",
    "final_col = [c for c in col if c not in drop_col]\n",
    "print \"Final features: \",final_col\n",
    "\n",
    "dfcc_monthly_dues_rdd = dfcc_monthly_dues.select(final_col).rdd\n",
    "train, test = dfcc_monthly_dues_rdd.map(\n",
    "                lambda line: parserPoint(line)).randomSplit(weights=[0.8,0.2])\n",
    "\n",
    "print \"\\n\"\n",
    "print \"Train: \",train.count()\n",
    "print \"Test : \",test.count()\n",
    "print \"sample --\"\n",
    "print train.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "logit_model = LogisticRegressionWithLBFGS.train(\n",
    "                    data=train, intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test err:  0.22725004159\n"
     ]
    }
   ],
   "source": [
    "predictions = test.map(lambda p: (p.label,\n",
    "                                  logit_model.predict(p.features)))\n",
    "test_err = predictions.filter(lambda (v,p):\n",
    "                              v!=p).count()/float(test.count())\n",
    "\n",
    "print \"Test err: \",test_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <td>843.870715</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>EDUCATION</th>\n",
       "      <td>134.471122</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MARRIAGE</th>\n",
       "      <td>33.346437</td>\n",
       "      <td>2.721829e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE_AMT1</th>\n",
       "      <td>19297.884229</td>\n",
       "      <td>8.469954e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE_AMT2</th>\n",
       "      <td>19089.437952</td>\n",
       "      <td>8.087560e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE_AMT3</th>\n",
       "      <td>18649.504659</td>\n",
       "      <td>8.942534e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE_AMT4</th>\n",
       "      <td>18426.948821</td>\n",
       "      <td>8.864715e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE_AMT5</th>\n",
       "      <td>17929.078264</td>\n",
       "      <td>8.943814e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE_AMT6</th>\n",
       "      <td>17736.333330</td>\n",
       "      <td>8.245731e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Statistic       p-value\n",
       "LIMIT_BAL    843.870715  0.000000e+00\n",
       "EDUCATION    134.471122  0.000000e+00\n",
       "MARRIAGE      33.346437  2.721829e-07\n",
       "DUE_AMT1   19297.884229  8.469954e-01\n",
       "DUE_AMT2   19089.437952  8.087560e-01\n",
       "DUE_AMT3   18649.504659  8.942534e-01\n",
       "DUE_AMT4   18426.948821  8.864715e-01\n",
       "DUE_AMT5   17929.078264  8.943814e-01\n",
       "DUE_AMT6   17736.333330  8.245731e-01"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "\n",
    "chi = Statistics.chiSqTest(train)\n",
    "\n",
    "\n",
    "records = [(result.statistic, \n",
    "            result.pValue) for result in chi]\n",
    "\n",
    "chi_df = pd.DataFrame(\n",
    "    data=records, index=final_col[:-1] , columns=[\"Statistic\",\"p-value\"])\n",
    "\n",
    "chi_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above it can be seen that only education, marriage and due_amt1 are important and rest can be removed from the model with out lossing the modelling power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model after removing categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final features:  ['LIMIT_BAL', 'DUE_AMT1', 'DUE_AMT2', 'DUE_AMT3', 'DUE_AMT4', 'DUE_AMT5', 'DUE_AMT6', 'DPNM']\n",
      "\n",
      "\n",
      "Train:  24056\n",
      "Test :  5944\n",
      "sample --\n",
      "(0.0,[60000.0,-25736.0,30500.0,0.0,-52829.0,11425.0,19003.0])\n"
     ]
    }
   ],
   "source": [
    "def parserPoint(line):\n",
    "    return LabeledPoint(line[-1], line[:-1])\n",
    "\n",
    "col = dfcc_monthly_dues.columns\n",
    "drop_col = ['ID','EDUCATION','MARRIAGE']\n",
    "final_col = [c for c in col if c not in drop_col]\n",
    "print \"Final features: \",final_col\n",
    "\n",
    "dfcc_monthly_dues_rdd = dfcc_monthly_dues.select(\n",
    "                        final_col).rdd\n",
    "train, test = dfcc_monthly_dues_rdd.map(\n",
    "                lambda line: parserPoint(line)).randomSplit(weights=[0.8,0.2])\n",
    "\n",
    "print \"\\n\"\n",
    "print \"Train: \",train.count()\n",
    "print \"Test : \",test.count()\n",
    "print \"sample --\"\n",
    "print train.first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.mllib.classification import LogisticRegressionWithLBFGS\n",
    "\n",
    "logit_model = LogisticRegressionWithLBFGS.train(\n",
    "                data=train, intercept=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test err:  0.215847913863\n"
     ]
    }
   ],
   "source": [
    "predictions = test.map(lambda p: (\n",
    "                p.label, logit_model.predict(p.features)))\n",
    "test_err = predictions.filter(\n",
    "            lambda (v,p): v!=p).count()/float(test.count())\n",
    "\n",
    "print \"Test err: \",test_err"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Hypothesis Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LIMIT_BAL', 'DUE_AMT1', 'DUE_AMT2', 'DUE_AMT3', 'DUE_AMT4', 'DUE_AMT5', 'DUE_AMT6', 'DPNM']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Statistic</th>\n",
       "      <th>p-value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>LIMIT_BAL</th>\n",
       "      <td>824.884543</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE_AMT1</th>\n",
       "      <td>19364.913137</td>\n",
       "      <td>0.875616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE_AMT2</th>\n",
       "      <td>19130.905306</td>\n",
       "      <td>0.706639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE_AMT3</th>\n",
       "      <td>18735.049287</td>\n",
       "      <td>0.812910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE_AMT4</th>\n",
       "      <td>18467.556856</td>\n",
       "      <td>0.876335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE_AMT5</th>\n",
       "      <td>17958.994734</td>\n",
       "      <td>0.916342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DUE_AMT6</th>\n",
       "      <td>17684.354716</td>\n",
       "      <td>0.843363</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Statistic   p-value\n",
       "LIMIT_BAL    824.884543  0.000000\n",
       "DUE_AMT1   19364.913137  0.875616\n",
       "DUE_AMT2   19130.905306  0.706639\n",
       "DUE_AMT3   18735.049287  0.812910\n",
       "DUE_AMT4   18467.556856  0.876335\n",
       "DUE_AMT5   17958.994734  0.916342\n",
       "DUE_AMT6   17684.354716  0.843363"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.mllib.stat import Statistics\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', 30)\n",
    "\n",
    "chi = Statistics.chiSqTest(train)\n",
    "\n",
    "\n",
    "records = [(result.statistic, result.pValue) for result in chi]\n",
    "\n",
    "chi_df = pd.DataFrame(data=records, \n",
    "                      index=final_col[:-1] , columns=[\"Statistic\",\"p-value\"])\n",
    "\n",
    "print final_col\n",
    "chi_df "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above we can conclude that that only Due_Amt1 which is generated using Bill_Amt1 and Pay_Amt1 \n",
    "are imporatnt features and other features can be dropped without loss of modelling power"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
