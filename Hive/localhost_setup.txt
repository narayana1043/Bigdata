
UNDER PROGRESS: MYSQL USER SETUP IS YET TO BE FINISHED
Minimum Requirements:
Java version >= 8

Steps:
For ease install Hadoop, Hive, Spark etc in your home directory.
Download Hadoop binaries into the home directory (My version: Hadoop 2.7.3)
Download Hive binaries into the Home directory (My version: Hive )
Download MySQL (My version: mysql-5.7.18) I have downloaded a mysql-5.7.18.dmg file and installed it in my machine

How hive works?
Hive comes with Derby an RDBMS to maintain its meta-store data. However Derby only supports only single user on Hive, so we need to install other RDBMS.
For the purpose of this requirement we will use MySQL which is commonly used with Hive.
Hive runs on the top of Hadoop, which means the data is spread across nodes and the meta data about this distribution of where actual data is sitting is maintained by MySQL in our case.

Local Machine Setup:
Install the Mysql
Need not install Hive and Hadoop as they are binaries
Set your Path variables by editing the ~/.bash_profile in your machine. Need to make sure your ~/.bash_profile is some what similar to the the below sample
# Setting PATH for MySQL
PATH="/usr/local/mysql/bin:${PATH}"
export PATH

# Setting PATH for hadoop
export HADOOP_HOME=/Users/vmarni/hadoop
export PATH=$PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
export LD_LIBRARY_PATH=/usr/local/hadoop/lib/native/:$LD_LIBRARY_PATH

# Setting PATH for hive
export HIVE_HOME=/Users/vmarni/hive
export PATH=$PATH:$HIVE_HOME/bin
4. Above we are trying to set Hadoop Home, Hive Home, and then adding the bin folders of Hadoop and Hive to our path variable
5. Start the MySQL server that you have downloaded. Kept it running always (Remember we are maintaining the hive meta-store using MySQL).
5. To start Hadoop services on your machine you need to setup a localhost and you should be able to do ssh localhost and be able to login with out a password.
If it might help think of localhost as your cluster
ssh-keygen -t rsa
Press enter for each line 
cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
chmod og-wx ~/.ssh/authorized_keys 
Run this ssh localhost if you are able to ssh into the local host successfully without the password.
Note: If you are not able to finish this step. Please pause and figure out and fix it.
you use exist command to come out of the local host
Always know on which environment you are. If you are not sure do ssh localhost and then do exist to come out of localhost this works even when you are in localhost environment.
6. Starting Hadoop Services
Before we start the Hadoop services for the first time let us configure Hadoop settings
We are editing core-site.xml and hdfs-site.xml in the following path HADOOP_HOME/etc/hadoop/
core-site.xml
<!-- Put site-specific property overrides in this file. -->

<configuration>
        <property>
                <name>fs.defaultFS</name>
                <value>hdfs://localhost:9000</value>
        </property>

        <!-- Temporary directory for hadoop file system -->
        <property>
                <name>hadoop.tmp.dir</name>
                <value>/Users/vmarni/hadoop/hdfs/tmp</value>
                <description>A base for other temporary directories.</description>
        </property>

</configuration>
The hadoop.tmp.dir is the new place where all the HDFS files sit in your local mode and below that path everything is in HDFS. Please specify the full path for this value.
For connivence I have create the folder under Hadoop home itself for the Hadoop filesystem. you can put them where ever you like.
Set the fs.default.FS as shown above
hdfs-site.xml

<!-- Put site-specific property overrides in this file. -->
<configuration>
        <property>
                <name>dfs.replication</name>
                <value>1</value>
        </property>
</configuration>

Please set this if it is missing int the hdfs-site.xml. Here we setting the replication to just one in the local cluster.

Try starting Hadoop services using the following commands
start-dfs.sh       This should start the name node and the data nodes
start-yarn.sh .   This should start the resource manager and node managers
For stopping the Hadoop services when you are done use
stop-dfs.sh
stop-yarn.sh
To check if all the Hadoop services are running properly just run jps and you should see some thing like below. We must make sure that things highlighted in red are there. If not stop here and fix things.
9232 SecondaryNameNode
9473 NodeManager
18052 Jps
551 
9369 ResourceManager
9004 NameNode
9103 DataNode
7. Configuring MySQL
Here we are configuring MySQL so that Hive can use MySQL to maintain its meta in the local machine.
Download .dmg file for MySQL and install it and then start the MySQL-server (IMPORTANT: Note the password for the root that is generated by default)
Once you start the My-SQL server you should be able to run mysql command on your machine
set the my-sql root password
run the following command and follow the instructions like below
mysql_secure_installation
Enter current password for root (enter for none):
OK, successfully used password, moving on...
[...]
Set root password? [Y/n] y
New password:
Re-enter new password:
Remove anonymous users? [Y/n] Y
[...]
Disallow root login remotely? [Y/n] N
[...]
Remove test database and access to it [Y/n] Y
[...]
Reload privilege tables now? [Y/n] Y
All done!

Then set up the meta-store database as follows

$ mysql -u root -p
Enter password:
mysql> CREATE DATABASE metastore;
mysql> USE metastore;
mysql> SOURCE ~/hive/scripts/metastore/upgrade/mysql/hive-schema-n.n.n.mysql.sql; (NOTE: n.n.n -> 2.1.0 latest which I am using) 
Make sure the last command has no errors (NOTE: Warning are fine here)
You also need a MySQL user account for Hive to use to access the metastore. It is very important to prevent this user account from creating or altering tables in the meta-store database schema.
mysql> CREATE USER 'hiveuser'@'localhost' IDENTIFIED BY 'hivepassword';
mysql> REVOKE ALL PRIVILEGES, GRANT OPTION FROM 'hiveuser'@'localhost';
mysql> GRANT ALL PRIVILEGES ON metastore.* TO 'hiveuser'@'localhost';
mysql> FLUSH PRIVILEGES;
Then quit MySQL prompt.
You also need to put the jar file(mysql-connector-java-5.1.28.jar) in the lib directory of Hive

8. Setting Up Hive
Before we start the hive service for the first time lets configure a few things.
Remember we have installed MySQL but not yet configured hive to use MySQL. So we will first set that in the following file available under the path HIVE_HOME/conf/hive-site.xml
Below I have pasted my config file for your reference
Things which we should focus
javax.jdo.option.ConnectionUserName
javax.jdo.option.ConnectionPassword
Note: 
To connect to MySQL you also need to have user account on MySQL on to which you can login.
In my case I am using "hiveuser" and "hivepassword"
hive-site.xml settings
<configuration>

<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:mysql://localhost/metastore?createDatabaseIfNotExist=true</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionUserName</name>
  <value>hiveuser</value>
</property>

<property>
  <name>javax.jdo.option.ConnectionPassword</name>
  <value>hivepassword</value>
</property>

<property>
  <name>datanucleus.autoCreateSchema</name>
  <value>true</value>
</property>

<property>
  <name>datanucleus.fixedDatastore</name>
  <value>true</value>
</property>

<property>
 <name>datanucleus.autoCreateTables</name>
 <value>True</value>
</property>

<property> REMOVE THIS IF IT IS THERE "AS OF NOW I AM NOT USER WHAT IS THIS AND WHY IS FOR? PLEASE IGNORE THIS. ANYWAYS HIVE SHOULD WORK IN LOCAL FOR YOU"
  <name>hive.metastore.uris</name>
  <value>thrift://localhost:9083</value>
  <description>IP address (or fully-qualified domain name) and port of the metastore host</description>
</property>

</configuration>

Things that need to be working before you start hive
you should be able to ssh localhost (check ssh localhost)
your mysql should be up and running on local host
type "jps" and check your Node manager, resource manager, data node and name node are all running in Hadoop (check start-dfs.sh, start-yarn.sh)
Now try to start hive on the command line using hive or beeline

Cheers! Hive should be working now.
